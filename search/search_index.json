{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"FYS-STK4155 Projects in Machine Learning Jacob, Jonas & Elias","title":"Home"},{"location":"index.html#fys-stk4155","text":"","title":"FYS-STK4155"},{"location":"index.html#projects-in-machine-learning","text":"","title":"Projects in Machine Learning"},{"location":"index.html#jacob-jonas-elias","text":"","title":"Jacob, Jonas &amp; Elias"},{"location":"project1.html","text":"Project 1 FYS-STK4155 Regression analysis and resampling methods In this project, we explore various regression methods. First, we consider a data set with added noise generated from the Franke function. Then we apply the same methods to predict contours from real terrain data. To benchmark the code, we find it most useful to test task g, in the script g.py. This produces contour plots and images from the terrain data. To speed up computations, try running up to a lower degree, e.g. degree = 20 on line 20. You can also try reducing data point, e.g from 50 to 30 (this speeds up computations a lot). These changes to the program already exists in g_test.py in the the test folder of this repository. Simply download the file and run > python3 g_test.py The program will output a lot of warnings, which comes from Sci-kit learn's modules. However, it should be fine as long as you don't encounter any errors. When the program is done running, it writes results to file. These are the lowest MSEs on the test data from the different regression and resampling methods.","title":"Project 1"},{"location":"project1.html#project-1-fys-stk4155","text":"","title":"Project 1 FYS-STK4155"},{"location":"project1.html#regression-analysis-and-resampling-methods","text":"In this project, we explore various regression methods. First, we consider a data set with added noise generated from the Franke function. Then we apply the same methods to predict contours from real terrain data. To benchmark the code, we find it most useful to test task g, in the script g.py. This produces contour plots and images from the terrain data. To speed up computations, try running up to a lower degree, e.g. degree = 20 on line 20. You can also try reducing data point, e.g from 50 to 30 (this speeds up computations a lot). These changes to the program already exists in g_test.py in the the test folder of this repository. Simply download the file and run > python3 g_test.py The program will output a lot of warnings, which comes from Sci-kit learn's modules. However, it should be fine as long as you don't encounter any errors. When the program is done running, it writes results to file. These are the lowest MSEs on the test data from the different regression and resampling methods.","title":"Regression analysis and resampling methods"},{"location":"project2.html","text":"Project 2 FYS-STK4155 Autumn 2020 Classification and Regression, from linear and logistic re-gression to neural networks In this project, we explore stochastic gradient descent and feedforward neural networks with back-propagation. First off, we wish to thank Elin Finstad and Anders Br\u00e5te for collaboration and great discussions. We also wish to thank the FYS-STK4155 team for all help. To run the programs we have made, download our repository and make sure you are in the src folder. cd src Now, you can run the scripts. We recommend starting off with analysing stochastic gradient descent on the Franke data set. You can do this by running the regression script: python3 regression.py Now your terminal will hopefully give you the options to analyse minibatches and epochs. Respond with Y for yes or n for no. The other option to analyse is learning schedule. Both analyses produce a heatmap and saves them in the folder figures . This program takes a few minutes to run. If you don't have 5 minutes, we recommend running the NN script. We have included a progress bar so you can track the progress. The next script we recommend to analyse is the neural network (NN), which analyses the Franke function when run. Run the following command python3 NN.py Now you will get following options to analyse learning rate vs epochs learning rate vs penalty parameter Keras (performs analysis of learning rate and penalty parameter using Keras library) relu activation (learning rate vs epochs) (changes activation function in hidden layers) Note that the analysis with Keras takes some time. The quickest analysis to perform is the first option, learning rate vs epochs. This is the analysis we recommend performing to test our code. All analyses produces heatmaps and saves them in the folder figures , and all analyses comes with a progress bar to track progress. The last script we have made easy to analyse is digit.py . This is the script which trains the network on the handwritten digits from MNIST. Run the program with python3 digit.py The program will by default produce a confusion matrix which shows how the network predicts digits. Optionally, you can analyse how learning rate and penalty penalty parameter changes the networks' performance. Simply press y and hit enter when the terminal asks you Analyse penalty parameter and learning rate [Y/n]: Keep in mind that different random seeds in the neural network affects results. The last program we have is log-reg.py , which performs logistic regression. This program uses logistic regression to analyse digits from MNIST. It produces a heatmap of accuracy from both scikit learn and our own neural network with no hidden layers. It also produces a confusion matrix from our network. To run this program: python3 log-reg.py All other scripts in the folder src contain support functions.","title":"Project 2"},{"location":"project2.html#project-2-fys-stk4155-autumn-2020","text":"","title":"Project 2 FYS-STK4155 Autumn 2020"},{"location":"project2.html#classification-and-regression-from-linear-and-logistic-re-gression-to-neural-networks","text":"In this project, we explore stochastic gradient descent and feedforward neural networks with back-propagation. First off, we wish to thank Elin Finstad and Anders Br\u00e5te for collaboration and great discussions. We also wish to thank the FYS-STK4155 team for all help. To run the programs we have made, download our repository and make sure you are in the src folder. cd src Now, you can run the scripts. We recommend starting off with analysing stochastic gradient descent on the Franke data set. You can do this by running the regression script: python3 regression.py Now your terminal will hopefully give you the options to analyse minibatches and epochs. Respond with Y for yes or n for no. The other option to analyse is learning schedule. Both analyses produce a heatmap and saves them in the folder figures . This program takes a few minutes to run. If you don't have 5 minutes, we recommend running the NN script. We have included a progress bar so you can track the progress. The next script we recommend to analyse is the neural network (NN), which analyses the Franke function when run. Run the following command python3 NN.py Now you will get following options to analyse learning rate vs epochs learning rate vs penalty parameter Keras (performs analysis of learning rate and penalty parameter using Keras library) relu activation (learning rate vs epochs) (changes activation function in hidden layers) Note that the analysis with Keras takes some time. The quickest analysis to perform is the first option, learning rate vs epochs. This is the analysis we recommend performing to test our code. All analyses produces heatmaps and saves them in the folder figures , and all analyses comes with a progress bar to track progress. The last script we have made easy to analyse is digit.py . This is the script which trains the network on the handwritten digits from MNIST. Run the program with python3 digit.py The program will by default produce a confusion matrix which shows how the network predicts digits. Optionally, you can analyse how learning rate and penalty penalty parameter changes the networks' performance. Simply press y and hit enter when the terminal asks you Analyse penalty parameter and learning rate [Y/n]: Keep in mind that different random seeds in the neural network affects results. The last program we have is log-reg.py , which performs logistic regression. This program uses logistic regression to analyse digits from MNIST. It produces a heatmap of accuracy from both scikit learn and our own neural network with no hidden layers. It also produces a confusion matrix from our network. To run this program: python3 log-reg.py All other scripts in the folder src contain support functions.","title":"Classification and Regression, from linear and logistic re-gression to neural networks"},{"location":"project3.html","text":"Project 3 FYS-STK4155 Autumn 2020 Senior developer: Jonas T. Faber, Junior developers: Jacob Lie & Elias R. Udn\u00e6s Convolutional Neural Networks on fruit recognition In this project, we analyse a dataset consisting of labelled images of fruit. The dataset is publically available through Kaggle here . To download the part of the dataset we use, and what is needed to run our codes: https://easyupload.io/qa0u65 (3.4 GB) Extract the folder images to the repository (Project3). If you can't access the dataset for whatever reason, or don't trust the website, contact us @ m.e.r.udnas@fys.uio.no and we'll send you a dropbox folder with the data. We use the Feed Forward Neural Network (FFNN) developed in FYS-STK Project 2 , and a Convolutional Neural Network (CNN) from tensorflow using Keras API. The CNN was made with 2D convolution layers (e.g. spatial convolution over images) with Conv2D layer . Selected results can be found under the folder results . Source code is found under the folder src , with documentation on how to run selected scripts.","title":"Project 3"},{"location":"project3.html#project-3-fys-stk4155-autumn-2020","text":"Senior developer: Jonas T. Faber, Junior developers: Jacob Lie & Elias R. Udn\u00e6s","title":"Project 3 FYS-STK4155 Autumn 2020"},{"location":"project3.html#convolutional-neural-networks-on-fruit-recognition","text":"In this project, we analyse a dataset consisting of labelled images of fruit. The dataset is publically available through Kaggle here . To download the part of the dataset we use, and what is needed to run our codes: https://easyupload.io/qa0u65 (3.4 GB) Extract the folder images to the repository (Project3). If you can't access the dataset for whatever reason, or don't trust the website, contact us @ m.e.r.udnas@fys.uio.no and we'll send you a dropbox folder with the data. We use the Feed Forward Neural Network (FFNN) developed in FYS-STK Project 2 , and a Convolutional Neural Network (CNN) from tensorflow using Keras API. The CNN was made with 2D convolution layers (e.g. spatial convolution over images) with Conv2D layer . Selected results can be found under the folder results . Source code is found under the folder src , with documentation on how to run selected scripts.","title":"Convolutional Neural Networks on fruit recognition"}]}